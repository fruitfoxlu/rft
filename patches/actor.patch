--- /tmp/openrlhf_pkg/openrlhf_orig/openrlhf-0.9.3.data/purelib/openrlhf/models/actor.py	2026-02-05 12:33:06.000000000 +0000
+++ /opt/conda/envs/rft/lib/python3.10/site-packages/openrlhf/models/actor.py	2026-02-19 20:58:24.310730182 +0000
@@ -59,13 +59,6 @@
             # Support multiple attention mechanism implementations
             attn_impl = attn_implementation
 
-            # Note: dschf is defined in function scope to avoid global effects
-            # https://huggingface.co/docs/transformers/deepspeed#non-trainer-deepspeed-integration
-            if ds_config is not None and ds_config["zero_optimization"]["stage"] == 3:
-                dschf = HfDeepSpeedConfig(ds_config)
-            else:
-                dschf = None
-
             # Determine torch dtype based on param_dtype parameter, default: bf16
             from openrlhf.utils.utils import convert_to_torch_dtype
 
@@ -73,14 +66,33 @@
 
             if load_in_4bit:
                 assert param_dtype == "bf16", "we only support bnb_4bit_compute_dtype = bf16"
-                nf4_config = BitsAndBytesConfig(
+                quant_config = BitsAndBytesConfig(
                     load_in_4bit=True,
                     bnb_4bit_quant_type="nf4",
                     bnb_4bit_use_double_quant=True,
                     bnb_4bit_compute_dtype=torch.bfloat16,
                 )
             else:
-                nf4_config = None
+                quant_config = None
+
+            # For MXFP4 pre-quantized models (e.g. gpt-oss), dequantize to bf16
+            # so training can proceed with standard gradient math.
+            _is_mxfp4_dequant = False
+            if quant_config is None:
+                from transformers import AutoConfig
+                _cfg = AutoConfig.from_pretrained(pretrain_or_model, trust_remote_code=True)
+                _qcfg = getattr(_cfg, "quantization_config", None)
+                if isinstance(_qcfg, dict) and _qcfg.get("quant_method") == "mxfp4":
+                    from transformers import Mxfp4Config
+                    quant_config = Mxfp4Config(dequantize=True)
+                    _is_mxfp4_dequant = True
+
+            # Note: dschf is defined in function scope to avoid global effects
+            # https://huggingface.co/docs/transformers/deepspeed#non-trainer-deepspeed-integration
+            if ds_config is not None and ds_config["zero_optimization"]["stage"] == 3 and not _is_mxfp4_dequant:
+                dschf = HfDeepSpeedConfig(ds_config)
+            else:
+                dschf = None
 
             if use_liger_kernel:
                 from liger_kernel.transformers import AutoLigerKernelForCausalLM
@@ -89,19 +101,31 @@
             else:
                 model_class = AutoModelForCausalLM
 
+            # For MXFP4 dequantized models with ZeRO-3: force device_map="cpu"
+            # so all parameters are real CPU tensors (not meta). Combined with
+            # dont_change_device=True in deepspeed.initialize(), this lets
+            # ZeRO-3 partition CPU-resident params and move only shards to GPU,
+            # avoiding OOM from moving the full model to GPU.
+            _device_map = device_map
+            if _is_mxfp4_dequant and ds_config is not None and ds_config["zero_optimization"]["stage"] == 3:
+                _device_map = "cpu"
+
             self.model = model_class.from_pretrained(
                 pretrain_or_model,
                 trust_remote_code=True,
                 attn_implementation=attn_impl,
-                quantization_config=nf4_config,
-                torch_dtype=torch_dtype,  # default: bf16
-                device_map=device_map,
+                quantization_config=quant_config,
+                torch_dtype=torch_dtype,
+                device_map=_device_map,
             )
 
             # LoRA
             if lora_rank > 0:
                 # https://github.com/huggingface/peft/issues/137
                 self.model.enable_input_require_grads()
+                # PEFT expects "all-linear" as a string keyword, not a list
+                if isinstance(target_modules, (list, tuple)) and len(target_modules) == 1 and target_modules[0] == "all-linear":
+                    target_modules = "all-linear"
                 lora_config = LoraConfig(
                     task_type=TaskType.CAUSAL_LM,
                     r=lora_rank,
@@ -140,6 +164,18 @@
             # Use `model.generate(use_cache=True)` instead.`
             self.model.config.use_cache = False
 
+            # For MXFP4 dequantized models with ZeRO-3: partition the model
+            # AFTER loading and LoRA application. deepspeed.zero.Init(module=...)
+            # sets ds_id on all params so deepspeed.initialize() skips both
+            # module.to(device) and _broadcast_model(), avoiding GPU OOM.
+            if _is_mxfp4_dequant and ds_config is not None and ds_config["zero_optimization"]["stage"] == 3:
+                import torch.distributed as _dist
+                if _dist.is_initialized():
+                    deepspeed.zero.Init(module=self.model, config_dict_or_path=ds_config)
+                    # Flush any pending CUDA errors from zero.Init to prevent
+                    # "unhandled cuda error" in subsequent NCCL operations.
+                    torch.cuda.synchronize()
+
             # packing samples using Flash Attention 2
             self.packing_samples = packing_samples
         else:
