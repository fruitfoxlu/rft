--- openrlhf-0.9.3.data/purelib/openrlhf/trainer/ray/ppo_actor.py	2026-02-05 12:33:06.000000000 +0000
+++ /opt/conda/envs/rft/lib/python3.10/site-packages/openrlhf/trainer/ray/ppo_actor.py	2026-02-21 17:15:15.396809137 +0000
@@ -136,15 +136,34 @@
                 )
                 for i, engine in enumerate(self.vllm_engines)
             ]
-            if use_ray:
-                import ray.util.collective as collective
+            # Synchronize CUDA and clear caches before creating sync group
+            torch.cuda.synchronize()
+            torch.cuda.empty_cache()
 
-                collective.init_collective_group(world_size=world_size, rank=0, backend=backend, group_name=group_name)
-                self._model_update_group = group_name
+            if use_ray:
+                # With --vllm_sync_with_ray, we use Ray object store for weight
+                # transfer instead of NCCL/collective broadcast. No process group
+                # or collective init needed -- just skip to ray.get(refs).
+                print(f"[INFO] Using Ray object store for actor-vLLM weight sync (no NCCL needed)")
             else:
+                # Clear any pending CUDA errors before NCCL communicator init.
+                import ctypes
+                try:
+                    _libcudart = ctypes.CDLL("libcudart.so")
+                    _cuda_err = _libcudart.cudaGetLastError()
+                    if _cuda_err != 0:
+                        print(f"[WARN] Cleared pending CUDA error code: {_cuda_err}")
+                        _libcudart.cudaDeviceSynchronize()
+                        _cuda_err2 = _libcudart.cudaGetLastError()
+                        print(f"[WARN] After sync, CUDA error code: {_cuda_err2}")
+                except Exception as e:
+                    print(f"[WARN] CUDA error clearing failed: {e}")
+
+                print(f"[INFO] Creating NCCL sync group: device={torch.cuda.current_device()}, world_size={world_size}")
                 self._model_update_group = stateless_init_process_group(
                     master_address, master_port, 0, world_size, torch.cuda.current_device()
                 )
+                print(f"[INFO] NCCL sync group created successfully")
 
             ray.get(refs)
 
@@ -250,6 +269,16 @@
         if vllm_kl is not None:
             experience.info["vllm_kl"] = vllm_kl.detach()
 
+        # Ratio tail diagnostics (stored by PolicyLoss.forward)
+        if hasattr(self.actor_loss_fn, '_log_ratio_max'):
+            experience.info["log_ratio_max"] = torch.tensor([self.actor_loss_fn._log_ratio_max])
+            experience.info["log_ratio_min"] = torch.tensor([self.actor_loss_fn._log_ratio_min])
+            experience.info["log_ratio_raw_max"] = torch.tensor([self.actor_loss_fn._log_ratio_raw_max])
+            experience.info["log_ratio_raw_min"] = torch.tensor([self.actor_loss_fn._log_ratio_raw_min])
+            experience.info["ratio_max"] = torch.tensor([self.actor_loss_fn._ratio_max])
+            experience.info["log_ratio_abs_p99"] = torch.tensor([self.actor_loss_fn._log_ratio_abs_p99])
+            experience.info["tokens_in_batch"] = torch.tensor([float(self.actor_loss_fn._tokens_in_batch)])
+
         if self.args.use_kl_loss:
             if self.args.init_kl_coef > 0:
                 kl = compute_approx_kl(
@@ -299,6 +328,71 @@
         status = {"policy_loss": actor_loss.detach().item(), "actor_lr": self.actor_scheduler.get_last_lr()[0]}
         if self.args.entropy_loss_coef is not None:
             status["entropy_loss"] = entropy_loss.detach().item()
+            # Entropy distribution stats (token-level)
+            ent = output.entropy[:, -experience.action_mask.shape[1]:]
+            ent_vals = ent[experience.action_mask.bool()].detach()
+            if ent_vals.numel() > 0:
+                status["entropy_mean"] = ent_vals.mean().item()
+                status["entropy_p10"] = ent_vals.quantile(0.1).item()
+                status["entropy_p90"] = ent_vals.quantile(0.9).item()
+
+        # Gradient norm from DeepSpeed engine (cached from step())
+        # Always add to status dict to avoid KeyError in micro-batch accumulation
+        grad_norm_val = 0.0
+        try:
+            ds_model = self.actor.model
+            if hasattr(ds_model, "get_global_grad_norm"):
+                gn = ds_model.get_global_grad_norm()
+                if gn is not None:
+                    grad_norm_val = float(gn)
+        except Exception:
+            pass
+        status["grad_norm"] = grad_norm_val
+
+        # Spike logging: dump prompt hashes when grad_norm exceeds threshold
+        SPIKE_THRESHOLD = 50.0
+        if grad_norm_val > SPIKE_THRESHOLD and torch.distributed.get_rank() == 0:
+            try:
+                import hashlib, json, time as _time
+                spike_log_path = os.environ.get("SPIKE_LOG_PATH", "/mnt/scratch/rft_metrics_20b/spike_log.jsonl")
+                # Extract prompt tokens (before action_mask starts) for each sample
+                # sequences: [batch, seq_len], action_mask: [batch, response_len]
+                seq = sequences.detach()
+                prompt_len = seq.shape[1] - action_mask.shape[1]
+                prompt_hashes = []
+                for i in range(seq.shape[0]):
+                    prompt_tokens = seq[i, :prompt_len].cpu().tolist()
+                    # Remove padding (0s) from the left
+                    prompt_tokens = [t for t in prompt_tokens if t != 0]
+                    h = hashlib.sha256(str(prompt_tokens).encode()).hexdigest()[:16]
+                    prompt_hashes.append(h)
+                # Deduplicate (all 8 samples from same prompt have same hash)
+                unique_hashes = list(dict.fromkeys(prompt_hashes))
+                spike_entry = {
+                    "timestamp": _time.time(),
+                    "grad_norm": grad_norm_val,
+                    "policy_loss": actor_loss.detach().item(),
+                    "prompt_hashes": unique_hashes,
+                    "n_samples": seq.shape[0],
+                    "prompt_len": prompt_len,
+                    "log_ratio_max": self.actor_loss_fn._log_ratio_max if hasattr(self.actor_loss_fn, '_log_ratio_max') else None,
+                    "log_ratio_raw_max": self.actor_loss_fn._log_ratio_raw_max if hasattr(self.actor_loss_fn, '_log_ratio_raw_max') else None,
+                }
+                with open(spike_log_path, "a") as f:
+                    f.write(json.dumps(spike_entry) + "\n")
+                print(f"[SPIKE] grad_norm={grad_norm_val:.1f} policy_loss={spike_entry['policy_loss']:.4f} "
+                      f"prompt_hashes={unique_hashes}", flush=True)
+            except Exception as e:
+                print(f"[WARN] Spike logging failed: {e}", flush=True)
+
+        # Advantage stats (per micro-batch)
+        adv_mask = experience.action_mask.bool()
+        adv_vals = advantages[adv_mask].detach().float()
+        if adv_vals.numel() > 0:
+            status["advantage_mean"] = adv_vals.mean().item()
+            status["advantage_std"] = adv_vals.std().item()
+            status["advantage_max"] = adv_vals.max().item()
+            status["advantage_min"] = adv_vals.min().item()
 
         # merge logs from info field
         for k, v in experience.info.items():
@@ -318,25 +412,54 @@
 
         torch.cuda.empty_cache()
         model = self.actor.model.module
+
+        # ==========================================
+        # FAST PATH: LoRA-only weight sync
+        # Only sync the small LoRA adapter params (~MBs) instead of all
+        # model params (~100s of GBs). Workers apply the LoRA delta on GPU.
+        # ==========================================
+        has_lora = hasattr(model, 'peft_config') or hasattr(model, 'active_adapters')
+        use_ray = getattr(self.strategy.args, "vllm_sync_with_ray", False)
+        if has_lora and use_ray:
+            self._broadcast_lora_to_vllm(model, cache_reset_refs)
+            return
+
         count, num_params = 0, len(list(model.named_parameters()))
+        # Collect pending refs from async weight updates (Ray path only).
+        # We must NOT call ray.get() inside the per-param loop because with
+        # ZeRO-3, all ranks must participate in AllGather for each parameter.
+        # If rank 0 blocks on ray.get() while other ranks advance to the next
+        # AllGather, the collectives desync and deadlock.
+        pending_ray_refs = []
+        # Keep ObjectRefs alive so the object store doesn't evict data
+        # before LLMRayActors fetch it via ray.get().
+        pending_object_refs = []
 
         def _broadcast_param(param, count, num_params):
-            use_ray = getattr(self.strategy.args, "vllm_sync_with_ray", False)
             # Fire all vllm engines for broadcast
             if torch.distributed.get_rank() == 0:
                 shape = param.shape if self.strategy.args.zero_stage != 3 else param.ds_shape
-                refs = [
-                    engine.update_weight.remote(name, dtype=param.dtype, shape=shape, empty_cache=count == num_params)
-                    for engine in self.vllm_engines
-                ]
 
                 if use_ray:
-                    import ray.util.collective as collective
-
-                    collective.broadcast(param.data, 0, group_name=self._model_update_group)
+                    weight_cpu = param.data.cpu()
+                    weight_ref = ray.put(weight_cpu)
+                    ref_bytes = weight_ref.binary()
+                    del weight_cpu
+                    pending_object_refs.append(weight_ref)
+                    for engine in self.vllm_engines:
+                        pending_ray_refs.append(
+                            engine.update_weight_from_ref.remote(
+                                name, dtype=param.dtype, shape=shape,
+                                weight_ref_bytes=ref_bytes, empty_cache=count == num_params,
+                            )
+                        )
                 else:
+                    refs = [
+                        engine.update_weight.remote(name, dtype=param.dtype, shape=shape, empty_cache=count == num_params)
+                        for engine in self.vllm_engines
+                    ]
                     self._model_update_group.broadcast(param.data, src=0, stream=torch.cuda.current_stream())
-                ray.get(refs)
+                    ray.get(refs)
 
         def _handle_cuda_ipc(param, count, num_params):
             from torch.multiprocessing.reductions import reduce_tensor
@@ -388,6 +511,81 @@
                     with deepspeed.zero.GatheredParameters([param], enabled=self.strategy.args.zero_stage == 3):
                         _handle_cuda_ipc(param, count, num_params)
 
+        # Wait for all async weight updates to complete (Ray path).
+        # This is done AFTER the param loop so rank 0 is never blocked
+        # during AllGather operations.
+        if pending_ray_refs:
+            if torch.distributed.get_rank() == 0:
+                print(f"[INFO] Waiting for {len(pending_ray_refs)} async weight updates to vLLM engines...")
+                ray.get(pending_ray_refs)
+                print(f"[INFO] All weight updates completed.")
+                del pending_object_refs  # Release object store refs
+
+        if cache_reset_refs:
+            ray.get(cache_reset_refs)
+        torch.cuda.empty_cache()
+        torch_dist_barrier_and_cuda_sync()
+
+    def _broadcast_lora_to_vllm(self, model, cache_reset_refs):
+        """Fast LoRA weight sync: only transfer adapter params (~MBs).
+
+        Instead of syncing all model params (240 GB for 120B models),
+        only sync requires_grad=True LoRA adapter params (~900 MB).
+        vLLM workers apply the delta: W += scaling * B @ A on GPU.
+        """
+        import time as _time
+
+        # Get LoRA scaling factor from PEFT config
+        scaling = 2.0  # default fallback
+        for obj in [model, getattr(model, 'base_model', None)]:
+            if obj is not None and hasattr(obj, 'peft_config'):
+                cfg = next(iter(obj.peft_config.values()))
+                scaling = float(cfg.lora_alpha) / float(cfg.r)
+                break
+
+        # Collect only requires_grad=True params (LoRA adapters).
+        # All ranks iterate the same params in the same order.
+        # Only rank 0 copies data; other ranks just participate in
+        # GatheredParameters AllGather (required by ZeRO-3).
+        lora_state = {}
+        t0 = _time.monotonic()
+
+        for name, param in model.named_parameters():
+            if not param.requires_grad:
+                continue  # All ranks skip frozen params - no AllGather needed
+
+            with deepspeed.zero.GatheredParameters([param], enabled=self.strategy.args.zero_stage == 3):
+                if torch.distributed.get_rank() == 0:
+                    lora_state[name] = param.data.cpu().clone()
+
+        if torch.distributed.get_rank() == 0:
+            t1 = _time.monotonic()
+            total_bytes = sum(t.numel() * t.element_size() for t in lora_state.values())
+            print(f"[INFO] LoRA sync: collected {len(lora_state)} adapter params "
+                  f"({total_bytes / 1e6:.1f} MB) in {t1 - t0:.1f}s (scaling={scaling})")
+
+            # Single ray.put for all LoRA params (~95 MB fits in object store).
+            # Pass the ObjectRef directly to .remote() â€” Ray automatically
+            # resolves ObjectRef arguments before the actor method executes,
+            # so the actor receives the deserialized dict. This avoids the
+            # ObjectRef.__await__() hang that occurs with reconstructed refs.
+            lora_ref = ray.put(lora_state)
+            del lora_state
+
+            refs = []
+            for i, engine in enumerate(self.vllm_engines):
+                print(f"[INFO] Dispatching apply_lora_update to engine {i}...", flush=True)
+                refs.append(engine.apply_lora_update.remote(lora_ref, scaling))
+            print(f"[INFO] All {len(refs)} apply_lora_update.remote() calls dispatched.",
+                  flush=True)
+
+            print(f"[INFO] Waiting for {len(refs)} vLLM engines to apply LoRA updates...")
+            ray.get(refs, timeout=600)
+            t2 = _time.monotonic()
+            print(f"[INFO] LoRA weight sync completed in {t2 - t0:.1f}s")
+
+            del lora_ref
+
         if cache_reset_refs:
             ray.get(cache_reset_refs)
         torch.cuda.empty_cache()
