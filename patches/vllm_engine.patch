--- a/openrlhf/trainer/ray/vllm_engine.py
+++ b/openrlhf/trainer/ray/vllm_engine.py
@@ -116,6 +116,22 @@
             args=(name, dtype, shape, empty_cache),
         )

+    async def update_weight_from_ref(self, name, dtype, shape, weight_ref_bytes, empty_cache=False):
+        """Update a single weight from a Ray object store reference.
+
+        Dispatches to workers via collective_rpc for proper TP handling.
+        """
+        return await self.llm.collective_rpc(
+            "update_weight_from_ray_ref",
+            args=(name, dtype, shape, weight_ref_bytes, empty_cache),
+        )
+
+    async def apply_lora_update(self, lora_state, scaling):
+        """Apply LoRA adapter weight deltas to the base model.
+
+        Dispatches to workers via collective_rpc. Each TP worker
+        computes delta = scaling * B @ A and applies it to its shard.
+        """
+        return await self.llm.collective_rpc(
+            "apply_lora_delta",
+            args=(lora_state, scaling),
+        )
+
     async def update_weight_cuda_ipc(self, name, dtype, shape, ipc_handles, empty_cache=False):
         return await self.llm.collective_rpc(
             "update_weight_cuda_ipc",
